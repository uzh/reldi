31.3.2015
Meeting between Tanja and Peter;  agreed about Google drive, Peter to post
specifications of the S3IT actions/costs.


9.4.2015
Tanja got access to EdX at https://studio.openedx.mnf.uzh.ch/ Within one
week, we should have temporary login for Maja (and Nikola)
Lay summary (needed to transfer the funds for the first year) ready
Agnes Kolmer accepted to be the project manager; she will upload the lay
summary, open a third-party account for the project and inform Tanja when
the fund transfer can be required

17.4.2015
Maja and Nikola got access to EdX studio

21.4.2015
Agnes submitted Lay summary (needed to transfer the money)
Lay summary is online: http://p3.snf.ch/project-160501

29.4. and 6.5.2015
Appartments for Maja and Nikola have been arranged at Accademia Apartments,
Langensteinenstrasse 2, 8057 Zürich.

28.5.2015
Cash payments of flat rates (for accommodation and meals) are possible.
Agnes prepared forms for the first payments at the beginning of June, when
Maja and Nikola arrive in Zurich.

5.6.2015
Meeting: Maja, Nikola, Peter, Tanja

Repository: GitHub open, potentially another closed one for sensitive data
Nikola to send names for the repositories to Peter
Maja and Tanja to set up accounts
Peter to set up repositories and add us as collaborators 
Peter to send a page with a tutorial 

General: All installations need to be easily recreatable. Peter suggests
Ansible for this  

Documentation: confluence vs. wikimedia - to be decided 

Data and tools:
Nikola to send the list of required installations with priorities,
documentation and dependencies   

Distance learning: EdX, but still to be decided 

8.6.2015
Sent specifications for two computers to be ordered by Roger Lehman

-------
Maja
MacBook Pro 13” (2015), 2.9GHz, 512 SSD (English keyboard) 1,832.06 CHF (6%
discount)
Thunderbolt to Gigabit Ethernet adapter 32.00 CHF
MiniDisplay to VGA adapter 32.00 CHF
60W MagSafe 2 power adapter 89.00 CHF

Nikola
MacBook Pro 13” (2015), 2.9GHz, 512 SSD (English keyboard) 1,832.06 CHF (6%
discount)
Thunderbolt to Gigabit Ethernet adapter 32.00 CHF
MiniDisplay to VGA adapter 32.00 CHF
60W MagSafe 2 power adapter 89.00 CHF
Magsafe to Magsafe2 adapter 11.00 CHF

###

In case there is no discount, we would opt for the following:

Maja
MacBook Pro 13” (2015), 2.9GHz, 512 SSD (English keyboard) 1,949.00 CHF
Thunderbolt to Gigabit Ethernet adapter 32.00 CHF
MiniDisplay to VGA adapter 32.00 CHF

Nikola
MacBook Pro 13” (2015), 2.9GHz, 512 SSD (English keyboard) 1,949.00 CHF
Thunderbolt to Gigabit Ethernet adapter 32.00 CHF
-------

8.6.2015
Sent specification of required installation:

------
EARLY REQUIREMENTS:

### WebAnno ###
- available at https://code.google.com/p/webanno/

### NoSkE ###
- available at http://nlp.fi.muni.cz/trac/noske/wiki/Downloads
- will need SSH access to use it

### Wikimedia wiki ###
- decided to go with it as we want to be consistent regarding the use of
- free software

### Worpress ###
- will be used for a linguistic stimuli repository, it could be that Maja
- will need some help with adapting it in a later phase
- our main site will probably be WP as well, no idea what is easier — two
- instances or two sites inside one instance (multisite)


TBD:

### data storage ###
- for now it seems that we will not need a closed git as the potentially
- problematic data will be available at WordPress and it seems not to
- require versioning control
- we opened the ReLDI git at https://github.com/nljubesi/reldi, to be
- discussed if an organization is the way to go (for representation
- purposes, “reldi" is already taken :-( )
- beside git we will need one additional storage option as we sometimes have
- large corpora and lexicons, GBs or even TBs in size, SSH/SCP sounds ok,
- but we are open to better solutions

### API wrappers ###
- I am listing here a part of the current Croatian pipeline (available from
- http://faust.ffzg.hr/nlpws/gui/index.php?menu=box) for tokenization,
- part-of-speech tagging and lemmatization as an example of what kind of a
- mess we want to wrap
- the content of our Makefile is the following:
        python hr_tokenizer.py < ${TAG_FILE} > ${TAG_FILE}.vert
        cat $(TAG_FILE) | ${HUNPOS_DIR}/hunpos-tag ${TRAIN_FILE}.hunpos >
${TAG_FILE}.tag
        awk '{if ($$0!=""){split($$0,a,"\t"); print a[1] "/" a[2]}else{print
"<s/>/Z"}}' ${TAG_FILE} | tr -d " " > ${TAG_FILE}.tagtemp
        ${CSTLEMMA_DIR}/cstlemma -L -i ${TAG_FILE}.tagtemp -f
${TRAIN_FILE}.cstpats -d ${TRAIN_FILE}.cstdict -eU -o ${TAG_FILE}.taglem
        rm ${TAG_FILE}.tagtemp
        sed -r -i 's/<s\/> .*? Z//g' ${TAG_FILE}.taglem
- there are two external dependencies (beside my tokenizer):
        - hunpos https://code.google.com/p/hunpos/downloads/list
        - CST https://github.com/kuhumcst/cstlemma
- it seems to me that a uniform interface, obtained with shell or python
- wrappers, could be the way to go
- people seem to use WebLicht for such tasks as well:
- http://weblicht.sfs.uni-tuebingen.de/weblichtwiki/index.php/Main_Page

### generic tool for building search forms for structured data (lexicons)
### ###
- such search forms can probably be easily built in web.py or Django, but we
- would be interested in a not-a-single-line-of-code approach to building
- simple, but diverse, search forms for structured data (lexicons) encoded
- in JSON, XML or similar

### shell tools ###
- we would need a web-based interface to
        - bash
        - python
        - R (with plotting)

### EdX ###
- to be discussed once we get acquainted with it

10.6.2015

Structure of each workshop:

Day 1 (afternoon)
intro
short participant presentations
Day 2
two morning sessions
one afternoon session
invited speaker
Day 3
two morning sessions
one afternoon session
invited speaker
Day 4 (morning)
discussions
wrap-up

Round 1:
“ReLDI 101” -- Introduction to resources and tools (to be held both on ZG1
and BG1 on day 2)
corpora
corpora description
using NoSkE for querying corpora
tools
using tools for annotating corpora
stimuli
using (and expanding) the stimuli repository
Corpus-driven methods in linguistic research (day 3 ZG1)
Experimental methods in linguistic research (day 3 BG1)
Round 2:
Statistical analysis of language data (R) (3 sessions)
Programming for linguists (python, transformations + counting) (3 sessions)
From theory to data and back (1 session)
How to take control over your data (versioning control, issue tracking,
formats)
Symbolic vs. statistical approaches to language processing (2 sessions)
Combining corpus-driven and experimental methods (1 session)
Anotacija kao lingvistička analiza (WebAnno, upute za anotatore, slaganje
između anotatora, crowdsourcing)
Automatska morfološka analiza
Automatska sintaksička analiza
Kad nas podaci iznenade

13.06.2015.

Document titled “Instrument repository” added to “ReLDI current”; it
explains the classification scheme for the repository of experimental
instruments (to be implemented in Wordpress)

14.06.2015.

One central WP with the following tabs (sections): 
Project description
Resources and tools
Experimental instruments
Teaching

First version written in EN, HR and SR to follow

16.06.2015.

Project email address (scopes.reldi@gmail.com) and Twitter account
(scopes_reldi) opened 

Power adapters, mini router and port adapter collected from Roger Lehman

17.06.2015.

Document titled “Courses” added to “ReLDI current”; it details the structure
of BG and ZG seminars

18.06.2015.

Computers and adapters collected from Roger Lehman

MTE DISCUSSION -- bold tagset?
App could just go over to Agp?
Rp and Rr should probably stay in V (as Vmr, Vmp?)
lemmatization? should we keep the V-A connection? if we conflate Agp and
App, then not for sure
Prepositions -- potentially remove the case
Ordinal numerals to adjectives?
Adjectival pronouns to adjectives?
Pronoun categories should be drastically reduced

23.06.2015.

- Git can not be used for file sharing, need the server file system for that
- we should put the following on Git:
	- set.hr -- already there
	- set.sr
	- statistical profile of a language
	- code for self-tailored tools
- most of the stuff should be in the central repository, but things that need a lot of work (like sethr and setsr) should probably be in their own repos, a Makefile in the central repo should make sure that they are cloned once somebody needs them

24.06.2015.

- statistical profile
	- grapheme / phoneme level
		- grapheme distribution
		- n-th order Markov distribution
		- PoS-conditioned distributions
	- syllable level
		- syllable distribution
		- syllable type distribution
	- word level
		- token#lemma#pos distribution
		- lemma#pos distribution
		- pos distribution
		- n-th order pos distribution
		- msd distribution
		- n-th order msd distribution
	- sentence level
		- sentence length distribution
